{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final\n",
    "\n",
    "## Reconhecimento de convicção através da fala para suporte ao ensino\n",
    "\n",
    "Disciplina: Computação Afetiva - IA369Y\n",
    "\n",
    "Prof. Paula D. P. Costa\n",
    "\n",
    "\n",
    "Alunos:\n",
    "\n",
    "    Diego Garrido\n",
    "    Fernanda Ferreira\n",
    "    Ricardo Keigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import glob\n",
    "from hmmlearn.hmm import GaussianHMM, MultinomialHMM\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados em formato csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_cer = glob.glob('features/treinamento/certeza/*.csv')\n",
    "datas_inc = glob.glob('features/treinamento/incerteza/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizar as características para cada um dos áudios e concatená-los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(datas):\n",
    "    feat_list = []\n",
    "    for file in datas:\n",
    "        df = pd.read_csv(file, sep=';')\n",
    "        #df1 = df[df.columns[2:4]]\n",
    "        df1 = df.loc[:, 'alphaRatio_sma3': 'F3amplitudeLogRelF0_sma3nz'].values\n",
    "        d = np.array(df1)\n",
    "        #print (d.shape)\n",
    "        feat_list.append(d)\n",
    "    lenghts = []\n",
    "    for i in range(len(feat_list)):\n",
    "        lenghts.append(len(feat_list[i]))\n",
    "        #print (df1)\n",
    "    f = np.vstack(feat_list)\n",
    "    return f, lenghts, feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cer, len_cer, list_fcer = select_features(datas_cer)\n",
    "f_inc, len_inc, list_finc = select_features(datas_inc)\n",
    "#f_control, len_control = select_features(datas_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Classificador HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados do treinamento: \n ConvergenceMonitor(history=[-1682049.9027937579, -1682049.179727497], iter=50,\n",
      "          n_iter=50, tol=0.01, verbose=False)\n",
      "Convergência: True\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model1 = GaussianHMM(n_components=15, covariance_type='diag', n_iter=50)\n",
    "model1.fit(f_cer, len_cer)\n",
    "print ('Dados do treinamento:', model1.monitor_)\n",
    "print ('Convergência:', model1.monitor_.converged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados do treinamento: \n ConvergenceMonitor(history=[-1818662.1069479825, -1818650.887878068], iter=50,\n",
      "          n_iter=50, tol=0.01, verbose=False)\n",
      "Convergência: True\n"
     ]
    }
   ],
   "source": [
    "model2 = GaussianHMM(n_components=15, covariance_type='diag', n_iter=50)\n",
    "model2.fit(f_inc, len_inc)\n",
    "print ('Dados do treinamento:', model2.monitor_)\n",
    "print ('Convergência:', model2.monitor_.converged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar o modelo treinado em pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model1_certeza.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model1, \"Model1_certeza.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model2_incerteza.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model2, \"Model2_incerteza.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemplo do Carregamento do modelo.\n",
    "model1 = joblib.load(\"Model1_certeza.pkl\")\n",
    "model2 = joblib.load(\"Model2_incerteza.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o modelo, utilize:\n",
    "    \n",
    "    prob1 = model1.score(list_vcer[val])\n",
    "    prob2 = model2.score(list_vcer[val])\n",
    "    if prob1 > prob2:\n",
    "        print (['certeza'])\n",
    "    else:\n",
    "        print (['incerteza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação do Classificador para cada um dos áudios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_v_cer = glob.glob('features/validacao/certeza/*.csv')\n",
    "datas_v_inc = glob.glob('features/validacao/incerteza/*.csv')\n",
    "\n",
    "v_cer, ven_cer, list_vcer = select_features(datas_v_cer)\n",
    "v_inc, ven_inc, list_vinc = select_features(datas_v_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar a quantidade de acerto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação para certeza\n",
      "taxa de acerto: 63.26530612244898 taxa de erro: 36.734693877551024\n",
      "Validação para incerteza\n",
      "taxa de acerto: 57.14285714285714 taxa de erro: 42.857142857142854\n"
     ]
    }
   ],
   "source": [
    "# Para os áudios classificados como CERTEZA\n",
    "def taxa_cer(list_vcer): #taxa de erro e acerto em porcentagem\n",
    "    erros1 = []\n",
    "    acertos1 = []\n",
    "    for val in range(len(list_vcer)):\n",
    "        prob1 = model1.score(list_vcer[val])\n",
    "        prob2 = model2.score(list_vcer[val])\n",
    "        if prob1 > prob2:\n",
    "            acertos1.append(['certeza'])\n",
    "        else:\n",
    "            erros1.append(['incerteza'])\n",
    "    taxa_ac1 = (len(acertos1))/len(list_vcer)*100\n",
    "    taxa_er1 = (len(erros1))/len(list_vcer)*100\n",
    "    print ('taxa de acerto:', taxa_ac1, 'taxa de erro:', taxa_er1)\n",
    "    return taxa_ac1, taxa_er1\n",
    "\n",
    "# Para os áudios classificados como INCERTEZA\n",
    "def taxa_inc(list_vinc): #taxa de erro e acerto em porcentagem\n",
    "    erros2 = []\n",
    "    acertos2 = []\n",
    "    for val2 in range(len(list_vinc)):\n",
    "        prob3 = model1.score(list_vinc[val2])\n",
    "        prob4 = model2.score(list_vinc[val2])\n",
    "        if prob3 > prob4:\n",
    "            erros2.append(['certeza'])\n",
    "        else:\n",
    "            acertos2.append(['incerteza'])\n",
    "    taxa_ac2 = (len(acertos2))/len(list_vinc)*100\n",
    "    taxa_er2 = (len(erros2))/len(list_vinc)*100\n",
    "    print ('taxa de acerto:', taxa_ac2, 'taxa de erro:', taxa_er2)\n",
    "    return taxa_ac2, taxa_er2\n",
    "\n",
    "\n",
    "print ('Validação para certeza')\n",
    "ac1, er1 = taxa_cer(list_vcer)\n",
    "\n",
    "print ('Validação para incerteza')\n",
    "ac2, er2 = taxa_inc(list_vinc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "### Audio de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature_test(path=''):\n",
    "    df = pd.read_csv(path, sep=';')\n",
    "    df1 = df.loc[:, 'alphaRatio_sma3': 'F3amplitudeLogRelF0_sma3nz'].values\n",
    "    d = np.array(df1)\n",
    "    #print (d.shape)\n",
    "    lenghts = []\n",
    "    for i in range(len(d)):\n",
    "        lenghts.append(len(d[i]))\n",
    "    #print (df1)\n",
    "    f = np.vstack(d)\n",
    "    return f, lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certeza\n"
     ]
    }
   ],
   "source": [
    "path_t = 'features_projeto_IA369Y/certeza/I1questao13.csv'\n",
    "#path_t2 = 'features_projeto_IA369Y/incerteza/I21questao11.csv'\n",
    "\n",
    "f_test, len_test = select_feature_test(path_t)\n",
    "\n",
    "\n",
    "# Teste das perguntas de controle\n",
    "prob1 = model1.score(f_test)\n",
    "prob2 = model2.score(f_test)\n",
    "\n",
    "if prob1 > prob2:\n",
    "    print ('certeza')\n",
    "else:\n",
    "    print ('incerteza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- https://hmmlearn.readthedocs.io/en/latest/index.html\n",
    "\n",
    "- https://github.com/tiagoft/course_audio\n",
    "\n",
    "- https://github.com/naxingyu/opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
